{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional glycoproteomics by integrated network assembly and partitioning <br>\n",
    "Griffin ME, Thompson JW, Xiao Y et al. <br>\n",
    "June 17, 2020\n",
    "\n",
    "This notebook runs the sites and regions script on all of the glycomics samples. There will be 5 parts to this analysis:\n",
    "1. Run the 293T, brain, and liver datasets separately. This will serve to calculate the total number of sites in each of the tissues.\n",
    "2. Use the multiple experiments sites and regions script to keep the ETD and EThcD runs separate and then calculate the overlap between them for 293T cells, brain, and liver.  \n",
    "3. Filter the input data by activation type and run the sites and regions separately for the 293T cells, brain, and liver. This will allow the calculation of both the total and stratified (i.e. regions vs sites with xx localization probability) number of sites and regions found by each activation method (note that HCD might actually be slightly overrepresented because there are 4 replicates instead of 2--HCD fragmentation was performed in both the ETD and EThcD runs).\n",
    "4. Use the multiple experiments sites and regions script on the brain/liver combined search to then calculate the site/region level overlap between the two tissues.\n",
    "5. Finally, use the multiple experiments sites and regions script for the BAP1 KO quantitative glycomics samples. First, the data will be separated by raw file to compare top speed to top N before averaging the top speeed/top N for the analysis. Then, run limma to find the significantly changing sites and normalize the ratios to their protien expression.\n",
    "\n",
    "The two python scripts are as follows:\n",
    "\n",
    "    SitesAndRegions.py\n",
    "    SitesAndRegionsMultiExperiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import python packages \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib_venn import venn2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import urllib.request, urllib.parse, urllib.error,urllib.request,urllib.error,urllib.parse\n",
    "\n",
    "#set up R environment and import R packages\n",
    "import rpy2\n",
    "import rpy2.ipython\n",
    "%load_ext rpy2.ipython\n",
    "%R require(limma)\n",
    "%R require(ibb)\n",
    "\n",
    "#magic function to show plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "#ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary files (PSMs files from Proteome Discoverer)\n",
    "\n",
    "#list the filenames\n",
    "fname0 = '293T_Glycomics_Full_PSMs.txt'\n",
    "fname1 = 'Brain_Glycomics_Full_PSMs.txt'\n",
    "fname2 = 'Liver_Glycomics_Full_PSMs.txt'\n",
    "fname3 = 'LiverBrain_Glycomics_Full_PSMs.txt'\n",
    "fnameq = 'BAP1KO_Glycomics_Full_PSMs.txt'\n",
    "fnamep = 'BAP1KO_ProteinExpression_Full_Proteins.txt' #for protein expression\n",
    "list_fnames = [fname0, fname1, fname2, fname3, fnameq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data for the sites and regions script\n",
    "\n",
    "SITE_NAME = \"GlcNAc502\" #define the mod name\n",
    "\n",
    "#make a list for naming the dfs\n",
    "list_dfs = ['293T', 'Brain', 'Liver', 'LiverBrain', 'Quant']\n",
    "\n",
    "#set up a for loop for completing the operations on all dfs\n",
    "for i in range(4):\n",
    "    locals()['df{}'.format(list_dfs[i])] = pd.read_csv(list_fnames[i], sep='\\t').rename(columns={\n",
    "        \"Spectrum File\":\"RawFile\",\"Master Protein Accessions\":\"Protein\",\"First Scan\":\"ScanNumber\",\n",
    "        \"Activation Type\":\"Fragmentation\"})\n",
    "    locals()['df{}'.format(list_dfs[i])]['Fragmentation'] = \\\n",
    "    locals()['df{}'.format(list_dfs[i])]['Fragmentation'].map(lambda x: x.split(' ', 1)[0])\n",
    "    locals()['df{}'.format(list_dfs[i])].Modifications.dropna(inplace=True)\n",
    "    locals()['df{}'.format(list_dfs[i])][\"NumMods\"] = locals()['df{}'.format(list_dfs[i])][\"Modifications\"].apply(\n",
    "        lambda s: s.count(SITE_NAME))\n",
    "    locals()['df{}'.format(list_dfs[i])] = \\\n",
    "    locals()['df{}'.format(list_dfs[i])][locals()['df{}'.format(list_dfs[i])][\"NumMods\"]>0]\n",
    "    locals()['df{}'.format(list_dfs[i])][\"Probabilities\"] = \\\n",
    "    locals()['df{}'.format(list_dfs[i])][\"ptmRS %s Site Probabilities\"%SITE_NAME].apply(\n",
    "        lambda s: \";\".join([str(round(float(v.split()[-1])/100, 3)) for v in s.split(\"; \")]))\n",
    "    locals()['df{}'.format(list_dfs[i])][\"Positions\"] = \\\n",
    "    locals()['df{}'.format(list_dfs[i])].apply(lambda x: \";\".join(\n",
    "        [str(int(v.split(\"(\")[1].split(\")\")[0])-1+x[\"Position in Protein\"]) for v in x[\n",
    "            \"ptmRS %s Site Probabilities\"%SITE_NAME].split(\"; \")]),axis=1)\n",
    "    \n",
    "#repeat for the BAP1 KO run (different SITE_NAME)\n",
    "Q_SITE_NAME = \"GlcNAc731\"\n",
    "\n",
    "i=4\n",
    "locals()['df{}'.format(list_dfs[i])] = pd.read_csv(list_fnames[i], sep='\\t').rename(columns={\n",
    "    \"Spectrum File\":\"RawFile\",\"Master Protein Accessions\":\"Protein\",\"First Scan\":\"ScanNumber\",\n",
    "    \"Activation Type\":\"Fragmentation\"})\n",
    "locals()['df{}'.format(list_dfs[i])]['Fragmentation'] = \\\n",
    "locals()['df{}'.format(list_dfs[i])]['Fragmentation'].map(lambda x: x.split(' ', 1)[0])\n",
    "locals()['df{}'.format(list_dfs[i])].Modifications.dropna(inplace=True)\n",
    "locals()['df{}'.format(list_dfs[i])][\"NumMods\"] = locals()['df{}'.format(list_dfs[i])][\"Modifications\"].apply(\n",
    "    lambda s: s.count(Q_SITE_NAME))\n",
    "locals()['df{}'.format(list_dfs[i])] = \\\n",
    "locals()['df{}'.format(list_dfs[i])][locals()['df{}'.format(list_dfs[i])][\"NumMods\"]>0]\n",
    "locals()['df{}'.format(list_dfs[i])][\"Probabilities\"] = \\\n",
    "locals()['df{}'.format(list_dfs[i])][\"ptmRS %s Site Probabilities\"%Q_SITE_NAME].apply(\n",
    "    lambda s: \";\".join([str(round(float(v.split()[-1])/100, 3)) for v in s.split(\"; \")]))\n",
    "locals()['df{}'.format(list_dfs[i])][\"Positions\"] = \\\n",
    "locals()['df{}'.format(list_dfs[i])].apply(lambda x: \";\".join(\n",
    "    [str(int(v.split(\"(\")[1].split(\")\")[0])-1+x[\"Position in Protein\"]) for v in x[\n",
    "        \"ptmRS %s Site Probabilities\"%Q_SITE_NAME].split(\"; \")]),axis=1)\n",
    "\n",
    "#choose columns to keep\n",
    "cols_to_keep = [\"RawFile\",\"Fragmentation\",\"ScanNumber\",\"Protein\",\"Positions\",\"Probabilities\",\"NumMods\"]\n",
    "\n",
    "for i in range(5):\n",
    "    locals()['df{}_table'.format(list_dfs[i])] = locals()['df{}'.format(list_dfs[i])][cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set All HCD Localization Probabilities Equal Across S/Ts ###\n",
    "\n",
    "#PD/ptmRS cannot handle glycan NLs, so the localization probabilities for HCD are patently wrong\n",
    "#because the glycan is almost always lost, the localization probability will be set to equal for all S/Ts\n",
    "\n",
    "#write a for loop to perform this for all dfs\n",
    "for i in list_dfs:\n",
    "    #count the number of S/Ts\n",
    "    locals()['df{}_table'.format(i)]['NumSites'] = locals()['df{}_table'.format(i)].Probabilities.str.count(';') + 1\n",
    "    \n",
    "    #calculate the probability at each site based on the number of S/Ts (round to 3 decimals as in PD)\n",
    "    locals()['df{}_table'.format(i)]['Prob'] = round(1/locals()['df{}_table'.format(i)].NumSites, 3)\n",
    "    \n",
    "    #define a function to make a repeating list of probability values based on the number of S/Ts\n",
    "    def repeat(row):\n",
    "        return [row.Prob]*row.NumSites\n",
    "    \n",
    "    #apply this function to each df\n",
    "    locals()['df{}_table'.format(i)]['ProbList'] = locals()['df{}_table'.format(i)].apply(repeat, axis=1)\n",
    "    \n",
    "    #transform the list into a semi-colon separated string for the sites and regions script\n",
    "    locals()['df{}_table'.format(i)]['ProbListFinal'] = locals()['df{}_table'.format(i)].ProbList.apply(\n",
    "        lambda s: ';'.join(map(str, s)))\n",
    "    \n",
    "    #replace the probability values for HCD \n",
    "    locals()['df{}_table'.format(i)].loc[locals()['df{}_table'.format(i)].Fragmentation == 'HCD', 'Probabilities'] = \\\n",
    "    locals()['df{}_table'.format(i)].ProbListFinal\n",
    "    \n",
    "    #remake the original table with the columns required for the sites and regions script\n",
    "    locals()['df{}_tableF'.format(i)] = locals()['df{}_table'.format(i)][cols_to_keep]\n",
    "    \n",
    "    #export final tables to tsv files\n",
    "    locals()['df{}_tableF'.format(i)].to_csv(\n",
    "        '{}_Preprocessed.txt'.format(i), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#run the sites and regions script for each experiment\n",
    "#filenames represent input output1 output2\n",
    "#input is the preprocessed Proteome Discoverer PSMs file\n",
    "#output1 contains the list of regions and maximum parsimonious number of O-GlcNAc sites \n",
    "#output2 is the best ms2 and localization probability for each site\n",
    "#both outputs have a mathcing region ID\n",
    "%run -i SitesAndRegions.py 293T_Preprocessed.txt maxparcon_293T.txt bestms2_293T.txt\n",
    "%run -i SitesAndRegions.py Brain_Preprocessed.txt maxparcon_Brain.txt bestms2_Brain.txt\n",
    "%run -i SitesAndRegions.py Liver_Preprocessed.txt maxparcon_Liver.txt bestms2_Liver.txt\n",
    "%run -i SitesAndRegions.py LiverBrain_Preprocessed.txt maxparcon_LiverBrain.txt bestms2_LiverBrain.txt\n",
    "%run -i SitesAndRegions.py Quant_Preprocessed.txt maxparcon_BAP1KO.txt bestms2_BAP1KO.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the sites and regions output into dfs and count the number of sites for each\n",
    "\n",
    "list_dfs2 = ['293T', 'Brain', 'Liver', 'LiverBrain', 'BAP1KO']\n",
    "\n",
    "for i in list_dfs2:\n",
    "    locals()['dfallsites_{}'.format(i)] = pd.read_csv('bestms2_{}.txt'.format(i), sep='\\t')\n",
    "    locals()['dfregions_{}'.format(i)] = pd.read_csv('maxparcon_{}.txt'.format(i), sep='\\t')\n",
    "    #remove sites/regions that match to two master proteins\n",
    "    #this seems to happen sometimes when there is real evidence of two isoforms\n",
    "    #all cases checked were already explained by other data, so they can be reasonably ignored\n",
    "    locals()['dfregions_{}'.format(i)] = locals()['dfregions_{}'.format(i)][\n",
    "        ~locals()['dfregions_{}'.format(i)].Protein.str.contains(';')]\n",
    "    locals()['dfallsites_{}'.format(i)] = locals()['dfallsites_{}'.format(i)][\n",
    "        ~locals()['dfallsites_{}'.format(i)].Protein.str.contains(';')]\n",
    "    \n",
    "#initialize an empty df\n",
    "df_numsites = pd.DataFrame(np.empty((5,5)))\n",
    "\n",
    "#calculate the number of sites, localized sites, and regions\n",
    "for i in range (5):\n",
    "    #set the column and index names\n",
    "    df_numsites.columns = [['Total Number of Sites', 'Localized Sites', 'Unlocalized Sites', 'Regions', \n",
    "                            'Percent Localized']]\n",
    "    df_numsites.rename(index={i:list_dfs2[i]}, inplace=True)\n",
    "    \n",
    "    #count the total number of sites\n",
    "    df_numsites.loc[list_dfs2[i], 'Total Number of Sites'] = \\\n",
    "    locals()['dfregions_{}'.format(list_dfs2[i])]['Min Sites'].sum()\n",
    "    \n",
    "    #count the number of localized sites\n",
    "    df_numsites.loc[list_dfs2[i], 'Localized Sites'] = len(\n",
    "        locals()['dfregions_{}'.format(list_dfs2[i])][\n",
    "            locals()['dfregions_{}'.format(list_dfs2[i])]['Site ID Constraints'].str.contains('of') == False])\n",
    "    df_numsites.loc[list_dfs2[i], 'Unlocalized Sites'] = locals()['dfregions_{}'.format(list_dfs2[i])][\n",
    "        locals()['dfregions_{}'.format(list_dfs2[i])][\n",
    "            'Site ID Constraints'].str.contains('of') == True]['Min Sites'].sum()\n",
    "    df_numsites.loc[list_dfs2[i], 'Regions'] = len(\n",
    "        locals()['dfregions_{}'.format(list_dfs2[i])][\n",
    "            locals()['dfregions_{}'.format(list_dfs2[i])]['Site ID Constraints'].str.contains('of') == True])\n",
    "    df_numsites.loc[list_dfs2[i], 'Percent Localized'] = \\\n",
    "    (df_numsites.loc[list_dfs2[i], 'Localized Sites'].to_numpy() / df_numsites.loc[\n",
    "        list_dfs2[i], 'Total Number of Sites'].to_numpy())*100\n",
    "\n",
    "df_numsites = df_numsites.astype(int) #convert to ints\n",
    "df_numsites.to_csv('TotalSitesAndRegionsAll.csv') #export \n",
    "df_numsites #inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the dfs to new dfs for part 2\n",
    "#make a new 'Experiment' column based on the raw file and export for the sites and regions multi-experiment script\n",
    "\n",
    "for i in list_dfs[0:4]:\n",
    "    locals()['df{}_tableF_2'.format(i)] = locals()['df{}_tableF'.format(i)].copy()\n",
    "    locals()['df{}_tableF_2'.format(i)].loc[\n",
    "        locals()['df{}_tableF_2'.format(i)].RawFile.str.contains('ETD'), 'Experiment'] = 'ETD'\n",
    "    locals()['df{}_tableF_2'.format(i)].loc[\n",
    "        locals()['df{}_tableF_2'.format(i)].RawFile.str.contains('EThcD'), 'Experiment'] = 'EThcD'\n",
    "    locals()['df{}_tableF_2'.format(i)].to_csv(\n",
    "        '{}_Preprocessed_2.txt'.format(i), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#run the sites and regions script for each experiment as above\n",
    "%run -i SitesAndRegionsMultiExperiment.py 293T_Preprocessed_2.txt maxparcon_293T_2.txt bestms2_293T_2.txt\n",
    "%run -i SitesAndRegionsMultiExperiment.py Brain_Preprocessed_2.txt maxparcon_Brain_2.txt bestms2_Brain_2.txt\n",
    "%run -i SitesAndRegionsMultiExperiment.py Liver_Preprocessed_2.txt maxparcon_Liver_2.txt bestms2_Liver_2.txt\n",
    "%run -i SitesAndRegionsMultiExperiment.py LiverBrain_Preprocessed_2.txt maxparcon_LiverBrain_2.txt \\\n",
    "bestms2_LiverBrain_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reimport the output\n",
    "for i in list_dfs2[0:4]:\n",
    "    locals()['dfallsites2_{}'.format(i)] = pd.read_csv('bestms2_{}_2.txt'.format(i), sep='\\t')\n",
    "    locals()['dfregions2_{}'.format(i)] = pd.read_csv('maxparcon_{}_2.txt'.format(i), sep='\\t')\n",
    "    #remove sites/regions that match to two master proteins\n",
    "    #this seems to happen sometimes when there is real evidence of two isoforms\n",
    "    #all cases checked were already explained by other data, so they can be reasonably ignored\n",
    "    locals()['dfregions2_{}'.format(i)] = locals()['dfregions2_{}'.format(i)][\n",
    "        ~locals()['dfregions2_{}'.format(i)].Protein.str.contains(';')]\n",
    "    locals()['dfallsites2_{}'.format(i)] = locals()['dfallsites2_{}'.format(i)][\n",
    "        ~locals()['dfallsites2_{}'.format(i)].Protein.str.contains(';')]\n",
    "    \n",
    "#add to the max parsimonious site constraints file whether each region is a site or a region\n",
    "for i in list_dfs2[0:4]:\n",
    "    locals()['dfregions2_{}'.format(i)].loc[\n",
    "        locals()['dfregions2_{}'.format(i)]['Site ID Constraints'].str.contains('of'), 'Type'] = 'Region'\n",
    "    locals()['dfregions2_{}'.format(i)].loc[\n",
    "        ~locals()['dfregions2_{}'.format(i)]['Site ID Constraints'].str.contains('of'), 'Type'] = 'Site'\n",
    "    \n",
    "#merge the two dfs together and count up how many sites are found in the ETD vs EThcD runs\n",
    "for i in list_dfs2[0:4]:\n",
    "    locals()['dfmerge2_{}'.format(i)] = locals()['dfallsites2_{}'.format(i)].merge(\n",
    "        locals()['dfregions2_{}'.format(i)], how='left', on='Region ID')\n",
    "    \n",
    "    #define a function to make a new column stating which experiment each region id belongs to and apply it to merged df\n",
    "    def experiment(x):\n",
    "        if x.Experiment.eq('ETD').all():\n",
    "            return 'ETD'\n",
    "        elif x.Experiment.eq('EThcD').all():\n",
    "            return 'EThcD'\n",
    "        else:\n",
    "            return 'Both'\n",
    "    locals()['df2_exp_{}'.format(i)] = locals()['dfmerge2_{}'.format(i)].groupby(['Region ID']).apply(experiment)\n",
    "    \n",
    "    #make a new column in the regions df with the experiments each region was found in\n",
    "    locals()['dfregions2_{}'.format(i)]['Experiment'] = locals()['df2_exp_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the dfs to new dfs for part 2\n",
    "#separate and export the data from the different fragmentation types separately\n",
    "\n",
    "list_frag = ['HCD', 'ETD', 'EThcD']\n",
    "\n",
    "for i in list_dfs:\n",
    "    locals()['df{}_tableF_3'.format(i)] = locals()['df{}_tableF'.format(i)].copy()\n",
    "    for j in list_frag:\n",
    "        locals()['df{}_tableF_3_{}'.format(i,j)] = locals()['df{}_tableF_3'.format(i)][\n",
    "            locals()['df{}_tableF_3'.format(i)].Fragmentation == j]\n",
    "        locals()['df{}_tableF_3_{}'.format(i,j)].to_csv('{}_{}_Preprocessed_3.txt'.format(i,j), sep='\\t', index=False)\n",
    "    locals()['df{}_tableF_3'.format(i)]['NumST'] = locals()['df{}_tableF_3'.format(i)].Probabilities.str.count(';') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#run the sites and regions script for each experiment\n",
    "%run -i SitesAndRegions.py 293T_ETD_Preprocessed_3.txt maxparcon_293T_ETD_3.txt bestms2_293T_ETD_3.txt\n",
    "%run -i SitesAndRegions.py 293T_EThcD_Preprocessed_3.txt maxparcon_293T_EThcD_3.txt bestms2_293T_EThcD_3.txt\n",
    "%run -i SitesAndRegions.py 293T_HCD_Preprocessed_3.txt maxparcon_293T_HCD_3.txt bestms2_293T_HCD_3.txt\n",
    "\n",
    "%run -i SitesAndRegions.py Brain_ETD_Preprocessed_3.txt maxparcon_Brain_ETD_3.txt bestms2_Brain_ETD_3.txt\n",
    "%run -i SitesAndRegions.py Brain_EThcD_Preprocessed_3.txt maxparcon_Brain_EThcD_3.txt bestms2_Brain_EThcD_3.txt\n",
    "%run -i SitesAndRegions.py Brain_HCD_Preprocessed_3.txt maxparcon_Brain_HCD_3.txt bestms2_Brain_HCD_3.txt\n",
    "\n",
    "%run -i SitesAndRegions.py Liver_ETD_Preprocessed_3.txt maxparcon_Liver_ETD_3.txt bestms2_Liver_ETD_3.txt\n",
    "%run -i SitesAndRegions.py Liver_EThcD_Preprocessed_3.txt maxparcon_Liver_EThcD_3.txt bestms2_Liver_EThcD_3.txt\n",
    "%run -i SitesAndRegions.py Liver_HCD_Preprocessed_3.txt maxparcon_Liver_HCD_3.txt bestms2_Liver_HCD_3.txt\n",
    "\n",
    "%run -i SitesAndRegions.py LiverBrain_ETD_Preprocessed_3.txt maxparcon_LiverBrain_ETD_3.txt bestms2_LiverBrain_ETD_3.txt\n",
    "%run -i SitesAndRegions.py LiverBrain_EThcD_Preprocessed_3.txt maxparcon_LiverBrain_EThcD_3.txt \\\n",
    "bestms2_LiverBrain_EThcD_3.txt\n",
    "%run -i SitesAndRegions.py LiverBrain_HCD_Preprocessed_3.txt maxparcon_LiverBrain_HCD_3.txt bestms2_LiverBrain_HCD_3.txt\n",
    "\n",
    "%run -i SitesAndRegions.py Quant_ETD_Preprocessed_3.txt maxparcon_BAP1KO_ETD_3.txt bestms2_BAP1KO_ETD_3.txt\n",
    "%run -i SitesAndRegions.py Quant_EThcD_Preprocessed_3.txt maxparcon_BAP1KO_EThcD_3.txt bestms2_BAP1KO_EThcD_3.txt\n",
    "%run -i SitesAndRegions.py Quant_HCD_Preprocessed_3.txt maxparcon_BAP1KO_HCD_3.txt bestms2_BAP1KO_HCD_3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reimport the output and process as before also adding back in whether each is a site or a region\n",
    "#also match the raw file/scan number back to the preprocessed df which has the number of S/Ts\n",
    "\n",
    "#rename dfQuant_tableF_3 to dfBAP1KO... to make the scipt match\n",
    "dfBAP1KO_tableF_3 = dfQuant_tableF_3\n",
    "\n",
    "for i in list_dfs2:\n",
    "    for j in list_frag:\n",
    "        locals()['dfallsites3_{}_{}'.format(i,j)] = pd.read_csv('bestms2_{}_{}_3.txt'.format(i, j), sep='\\t')\n",
    "        locals()['dfregions3_{}_{}'.format(i,j)] = pd.read_csv('maxparcon_{}_{}_3.txt'.format(i, j), sep='\\t')\n",
    "        locals()['dfallsites3_{}_{}'.format(i,j)] = locals()['dfallsites3_{}_{}'.format(i,j)][\n",
    "            ~locals()['dfallsites3_{}_{}'.format(i,j)].Protein.str.contains(';')]\n",
    "        locals()['dfregions3_{}_{}'.format(i,j)] = locals()['dfregions3_{}_{}'.format(i,j)][\n",
    "            ~locals()['dfregions3_{}_{}'.format(i,j)].Protein.str.contains(';')]\n",
    "        locals()['dfregions3_{}_{}'.format(i,j)].loc[\n",
    "            locals()['dfregions3_{}_{}'.format(i,j)]['Site ID Constraints'].str.contains('of'), 'Type'] = 'Region'\n",
    "        locals()['dfregions3_{}_{}'.format(i,j)].loc[\n",
    "            ~locals()['dfregions3_{}_{}'.format(i,j)]['Site ID Constraints'].str.contains('of'), 'Type'] = 'Site'\n",
    "        locals()['dfmerge3_{}_{}'.format(i,j)] = locals()['dfallsites3_{}_{}'.format(i,j)].merge(\n",
    "            locals()['dfregions3_{}_{}'.format(i,j)], how='left', on='Region ID')\n",
    "        locals()['dfmerge3_{}_{}'.format(i,j)].rename(\n",
    "            columns={'Best Raw File':'RawFile', 'Best Scan Number':'ScanNumber'}, inplace=True)\n",
    "        locals()['dfmerge3_{}_{}'.format(i,j)] = locals()['dfmerge3_{}_{}'.format(i,j)].merge(\n",
    "            locals()['df{}_tableF_3'.format(i)], how='left', on=['RawFile', 'ScanNumber'])\n",
    "        locals()['dfmerge3_{}_{}'.format(i,j)].drop(\n",
    "            columns={'Protein_y', 'Fragmentation', 'Protein', 'Positions', 'Probabilities', 'NumMods'}, inplace=True)\n",
    "        locals()['dfmerge3_{}_{}'.format(i,j)].rename(columns={'Protein_x':'Protein'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the numbers of sites and regions in each df (see above for making dfs)\n",
    "for i in range(5):\n",
    "    locals()['df_numsites3_{}'.format(list_dfs2[i])] = pd.DataFrame(np.empty((3,7)))\n",
    "    for j in range(3):\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].columns = ['Total Number of Sites', 'Localized Sites', \n",
    "                                                                     'Average Localization Probability', 'Single ST', \n",
    "                                                                     'Unlocalized Sites', 'Regions', 'Percent Localized']\n",
    "        locals()['dftemp3_{}_{}'.format(list_dfs2[i], list_frag[j])] = \\\n",
    "        locals()['dfmerge3_{}_{}'.format(list_dfs2[i], list_frag[j])][['Region ID', 'NumST']]\n",
    "        locals()['dftemp3_{}_{}'.format(list_dfs2[i], list_frag[j])].drop_duplicates(inplace=True)\n",
    "        locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])] = \\\n",
    "        locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])].merge(\n",
    "            locals()['dftemp3_{}_{}'.format(list_dfs2[i], list_frag[j])], how='left', on='Region ID')\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].rename(index={j:list_frag[j]}, inplace=True)\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Total Number of Sites'] = \\\n",
    "        locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])]['Min Sites'].sum()\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Localized Sites'] = len(\n",
    "            locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])][\n",
    "                (locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])].Type == 'Site') & \\\n",
    "                (locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])].NumST != 1)])\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Average Localization Probability'] = \\\n",
    "        (locals()['dfmerge3_{}_{}'.format(list_dfs2[i], list_frag[j])][\n",
    "            (locals()['dfmerge3_{}_{}'.format(list_dfs2[i], list_frag[j])].Type == 'Site') & \\\n",
    "            (locals()['dfmerge3_{}_{}'.format(list_dfs2[i], list_frag[j])].NumST != 1)]['Best Probability'].mean())*100\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Single ST'] = len(\n",
    "            locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])][\n",
    "                locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])].NumST == 1])\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Unlocalized Sites'] = \\\n",
    "        locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])][\n",
    "            locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])].Type == 'Region']['Min Sites'].sum()\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Regions'] = len(\n",
    "            locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])][\n",
    "                locals()['dfregions3_{}_{}'.format(list_dfs2[i],list_frag[j])].Type == 'Region'])\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Percent Localized'] = \\\n",
    "        (locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Localized Sites'] / \\\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])].loc[list_frag[j], 'Total Number of Sites'])*100\n",
    "        locals()['df_numsites3_{}'.format(list_dfs2[i])] = locals()['df_numsites3_{}'.format(list_dfs2[i])].astype({\n",
    "            'Total Number of Sites':int, 'Localized Sites':int, 'Single ST':int, 'Unlocalized Sites':int, 'Regions':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the previous info to csv  files\n",
    "for i in list_dfs2:\n",
    "    locals()['df_numsites3_{}'.format(i)].to_csv('SitesAndRegionsByFrag_{}.csv'.format(i))\n",
    "\n",
    "display(df_numsites3_293T)\n",
    "display(df_numsites3_Brain)\n",
    "display(df_numsites3_Liver)\n",
    "display(df_numsites3_LiverBrain)\n",
    "display(df_numsites3_BAP1KO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the overlap between ETD and EThcD localized sites only\n",
    "#first make sets of the sites\n",
    "\n",
    "#make a new list for just ETD and EThcD\n",
    "list_frag2 = ['ETD', 'EThcD']\n",
    "\n",
    "#concatentate the protein and position ot make a unique site identifier\n",
    "#make sets out of these unique site identifiers for each sample\n",
    "for i in list_dfs2:\n",
    "    for j in list_frag2:\n",
    "        locals()['dfmerge3_{}_{}'.format(i,j)]['Site'] = \\\n",
    "        locals()['dfmerge3_{}_{}'.format(i,j)].Protein.str.cat(\n",
    "            locals()['dfmerge3_{}_{}'.format(i,j)].Position.astype(str), sep='@')\n",
    "        locals()['set3_{}_{}'.format(i,j)] = set(\n",
    "            locals()['dfmerge3_{}_{}'.format(i,j)][(locals()['dfmerge3_{}_{}'.format(i,j)].Type == 'Site') & \\\n",
    "                                                  (locals()['dfmerge3_{}_{}'.format(i,j)].NumST != 1)].Site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the final table from part three I will make a separate column for 'Experiment' based on the raw file\n",
    "\n",
    "#copy the df from part 3 into a new df\n",
    "dfLiverBrain_tableF_4 = dfLiverBrain_tableF_3.copy()\n",
    "\n",
    "#add a new experiment column based on the raw file and export for SitesAndRegions script\n",
    "dfLiverBrain_tableF_4.loc[dfLiverBrain_tableF_4.RawFile.str.contains('Brain'), 'Experiment'] = 'Brain'\n",
    "dfLiverBrain_tableF_4.loc[dfLiverBrain_tableF_4.RawFile.str.contains('Liver'), 'Experiment'] = 'Liver'\n",
    "dfLiverBrain_tableF_4.to_csv('SiteOverlap_LiverBrain_Preprocessed_4.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#run the sites and regions script\n",
    "%run -i SitesAndRegionsMultiExperiment.py SiteOverlap_LiverBrain_Preprocessed_4.txt maxparcon_LiverBrainOverlap_4.txt \\\n",
    "bestms2_LiverBrainOverlap_4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the output and process as before (making sets to calculate overlap)\n",
    "dfallsites4 = pd.read_csv('bestms2_LiverBrainOverlap_4.txt', sep='\\t')\n",
    "dfregions4 = pd.read_csv('maxparcon_LiverBrainOverlap_4.txt', sep='\\t')\n",
    "dfallsites4 = dfallsites4[~dfallsites4.Protein.str.contains(';')]\n",
    "dfregions4 = dfregions4[~dfregions4.Protein.str.contains(';')]\n",
    "\n",
    "dfregions4.loc[dfregions4['Site ID Constraints'].str.contains('of'), 'Type'] = 'Region'\n",
    "dfregions4.loc[~dfregions4['Site ID Constraints'].str.contains('of'), 'Type'] = 'Site'\n",
    "\n",
    "dfmerge4 = dfallsites4.merge(dfregions4, how='left', on='Region ID')\n",
    "\n",
    "def experimentO(x):\n",
    "    if x.Experiment.eq('Brain').all():\n",
    "        return 'Brain'\n",
    "    elif x.Experiment.eq('Liver').all():\n",
    "        return 'Liver'\n",
    "    else:\n",
    "        return 'Both'\n",
    "df4_exp = dfmerge4.groupby(['Region ID']).apply(experimentO)\n",
    "dfregions4['Experiment'] = df4_exp\n",
    "\n",
    "series4_sites = dfregions4[dfregions4.Type == 'Site'].groupby('Experiment')['Min Sites'].sum()\n",
    "series4_sr = dfregions4.groupby('Experiment')['Min Sites'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-calculate the sites and regions for the brain and liver based on this combined dataset\n",
    "\n",
    "#note, use this with caution, since the sites are counted as a site if they are a site in either of the two datasets\n",
    "#(although this is probably mostly alright, it is perfectly reasonable that the sites localized in the brain are not  \n",
    "    #the same as some of the sites that were unlocalized in the liver)\n",
    "\n",
    "#initialize an empty df\n",
    "df_numsites4 = pd.DataFrame(np.empty((2,5)))\n",
    "\n",
    "#make a list for iterating over liver and brain\n",
    "list_lb = ['Brain', 'Liver']\n",
    "\n",
    "#make a for loop to go through liver and brain\n",
    "for i in range(2):\n",
    "    df_numsites4.columns = ['Total Number of Sites', 'Localized Sites', 'Unlocalized Sites', 'Regions', \n",
    "                            'Percent Localized']\n",
    "    df_numsites4.rename(index={i:list_lb[i]}, inplace=True)\n",
    "    df_numsites4.loc[list_lb[i], 'Total Number of Sites'] = dfregions4[\n",
    "        (dfregions4.Experiment == '{}'.format(list_lb[i])) | (dfregions4.Experiment == 'Both')]['Min Sites'].sum()\n",
    "    df_numsites4.loc[list_lb[i], 'Localized Sites'] = dfregions4[\n",
    "        ((dfregions4.Experiment == '{}'.format(list_lb[i])) | (dfregions4.Experiment == 'Both')) & \\\n",
    "        (dfregions4.Type == 'Site')]['Min Sites'].sum()\n",
    "    df_numsites4.loc[list_lb[i], 'Unlocalized Sites'] = dfregions4[\n",
    "        ((dfregions4.Experiment == '{}'.format(list_lb[i])) | (dfregions4.Experiment == 'Both')) & \\\n",
    "        (dfregions4.Type == 'Region')]['Min Sites'].sum()\n",
    "    df_numsites4.loc[list_lb[i], 'Regions'] = len(\n",
    "        dfregions4[((dfregions4.Experiment == '{}'.format(list_lb[i])) | (dfregions4.Experiment == 'Both')) & \\\n",
    "                   (dfregions4.Type == 'Region')])\n",
    "\n",
    "df_numsites4['Percent Localized'] = (df_numsites4['Localized Sites'] / df_numsites4['Total Number of Sites'])*100\n",
    "df_numsites4 = df_numsites4.astype({'Total Number of Sites':int, 'Localized Sites':int, 'Unlocalized Sites':int, \n",
    "                                   'Regions':int})\n",
    "df_numsites4.to_csv('SitesAndRegionsByTissue_LiverBrain.csv')\n",
    "df_numsites4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a separate column for 'Experiment' based on the raw file as in part 4 using the final table from part three\n",
    "\n",
    "#copy the df from part 3 into a new df\n",
    "dfQuant_tableF_5 = dfQuant_tableF_3.copy()\n",
    "\n",
    "#make a list of experiment types\n",
    "list_mstype = ['TopN', 'TopSpeed']\n",
    "\n",
    "#add a new experiment column based on the raw file and export for SitesAndRegions script\n",
    "for i in range(3):\n",
    "    for j in list_mstype:\n",
    "        dfQuant_tableF_5.loc[(dfQuant_tableF_5.RawFile.str.contains('_{}_'.format(i+1)) & \\\n",
    "                              dfQuant_tableF_5.RawFile.str.contains(\n",
    "            '{}'.format(j))), 'Experiment'] = '{}{}'.format(j,i+1)\n",
    "\n",
    "dfQuant_tableF_5.to_csv('Quant_Preprocessed_ByRawFile_5.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#run the sites and regions script\n",
    "%run -i SitesAndRegionsMultiExperiment.py Quant_Preprocessed_ByRawFile_5.txt maxparcon_BAP1KO_ByRawFile_5.txt \\\n",
    "bestms2_BAP1KO_ByRawFile_5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the output and process as before\n",
    "dfallsites5 = pd.read_csv('bestms2_BAP1KO_ByRawFile_5.txt', sep='\\t')\n",
    "dfregions5 = pd.read_csv('maxparcon_BAP1KO_ByRawFile_5.txt', sep='\\t')\n",
    "dfallsites5 = dfallsites5[~dfallsites5.Protein.str.contains(';')]\n",
    "dfregions5 = dfregions5[~dfregions5.Protein.str.contains(';')]\n",
    "\n",
    "dfregions5.loc[dfregions5['Site ID Constraints'].str.contains('of'), 'Type'] = 'Region'\n",
    "dfregions5.loc[~dfregions5['Site ID Constraints'].str.contains('of'), 'Type'] = 'Site'\n",
    "\n",
    "dfmerge5 = dfallsites5.merge(dfregions5, how='left', on='Region ID')\n",
    "\n",
    "#clean up this df\n",
    "dfmerge5.rename(columns={'Protein_x': 'Protein'}, inplace=True)\n",
    "dfmerge5.drop(columns={'Protein_y'}, inplace=True)\n",
    "\n",
    "#add whether sites/regions were found in top n, top speed, or both to the regions df\n",
    "def experimentR(x):\n",
    "    if x.Experiment.str.contains('TopSpeed').all():\n",
    "        return 'TopSpeed'\n",
    "    elif x.Experiment.str.contains('TopN').all():\n",
    "        return 'TopN'\n",
    "    else:\n",
    "        return 'Both'\n",
    "df5_exp = dfmerge5.groupby(['Region ID']).apply(experimentR)\n",
    "dfregions5['Experiment'] = df5_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the MS3 data from the psms file\n",
    "\n",
    "#reimport the psms file\n",
    "df5_psm = pd.read_csv(fnameq, sep='\\t')\n",
    "\n",
    "#pull out just the relevant information\n",
    "df5_psm = df5_psm[['Spectrum File', 'First Scan', 'Abundance 127N', 'Abundance 129C', 'Abundance 130N']]\n",
    "\n",
    "#rename the columns to facilitate merging\n",
    "df5_psm.rename(columns={'Spectrum File':'Best Raw File', 'First Scan':'Best Scan Number'}, inplace=True)\n",
    "\n",
    "#now merge this with the sites and regions data frame on raw file and scan number\n",
    "df5_quant = dfmerge5.merge(df5_psm, how='left', on=['Best Raw File', 'Best Scan Number'])\n",
    "\n",
    "#normalize the abundances based on the summed abundance for each channel/raw file\n",
    "for i in set(df5_quant['Best Raw File']):\n",
    "    \n",
    "    #127N\n",
    "    df5_quant.loc[df5_quant['Best Raw File'] == i, 'Norm-127N'] = \\\n",
    "    df5_quant.loc[df5_quant['Best Raw File'] == i, 'Abundance 127N'] / df5_quant.loc[\n",
    "        df5_quant['Best Raw File'] == i, 'Abundance 127N'].sum()\n",
    "    \n",
    "    #129C\n",
    "    df5_quant.loc[df5_quant['Best Raw File'] == i, 'Norm-129C'] = \\\n",
    "    df5_quant.loc[df5_quant['Best Raw File'] == i, 'Abundance 129C'] / df5_quant.loc[\n",
    "        df5_quant['Best Raw File'] == i, 'Abundance 129C'].sum()\n",
    "    \n",
    "    #130N\n",
    "    df5_quant.loc[df5_quant['Best Raw File'] == i, 'Norm-130N'] = \\\n",
    "    df5_quant.loc[df5_quant['Best Raw File'] == i, 'Abundance 130N'] / df5_quant.loc[\n",
    "        df5_quant['Best Raw File'] == i, 'Abundance 130N'].sum()\n",
    "\n",
    "#take the ratios based on the experiment\n",
    "df5_quant.loc[df5_quant.Experiment.str.contains('1'), 'Ratio'] = \\\n",
    "df5_quant.loc[df5_quant.Experiment.str.contains('1'), 'Norm-130N'] / df5_quant.loc[\n",
    "    df5_quant.Experiment.str.contains('1'), 'Norm-127N']\n",
    "\n",
    "df5_quant.loc[df5_quant.Experiment.str.contains('2'), 'Ratio'] = \\\n",
    "df5_quant.loc[df5_quant.Experiment.str.contains('2'), 'Norm-129C'] / df5_quant.loc[\n",
    "    df5_quant.Experiment.str.contains('2'), 'Norm-127N']\n",
    "\n",
    "df5_quant.loc[df5_quant.Experiment.str.contains('3'), 'Ratio'] = \\\n",
    "df5_quant.loc[df5_quant.Experiment.str.contains('3'), 'Norm-130N'] / df5_quant.loc[\n",
    "    df5_quant.Experiment.str.contains('3'), 'Norm-127N']\n",
    "\n",
    "#make a new trial column based on the experiment (to facilitate averaging top speed and top n)\n",
    "for i in range(3):\n",
    "    df5_quant.loc[df5_quant.Experiment.str.contains('{}'.format(i+1)), 'Trial'] = 'Trial{}'.format(i+1)\n",
    "    \n",
    "#average the ratios of top speed and top n trials and merge this into the regions df\n",
    "dfratios5 = dfregions5.merge(\n",
    "    df5_quant.groupby(['Region ID', 'Trial']).Ratio.mean().unstack().reset_index(), how='left', on='Region ID')\n",
    "\n",
    "#set this up for the limma script and output to txt\n",
    "dflimma5 = dfratios5[['Trial1', 'Trial2', 'Trial3']]\n",
    "dflimma5 = np.log2(dflimma5)\n",
    "dflimma5.to_csv('BAP1KO_SitesRegions_Ratios_Limma.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#calculate the p-values for changing sites with limma in R\n",
    "\n",
    "library(limma) #load the package\n",
    "df1 = read.table(\"BAP1KO_SitesRegions_Ratios_Limma.txt\",sep=\"\\t\",header=TRUE,row.names=1) #read in the file\n",
    "f1 = eBayes(lmFit(df1)) #perform limma\n",
    "write.table(topTable(f1,n=Inf,confint=.95),\"BAP1KO_limmaOut.txt\",sep=\"\\t\") #write table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the limma results and integrate them into the ratios table\n",
    "\n",
    "df5_limmaout = pd.read_csv('BAP1KO_limmaOut.txt', sep='\\t')\n",
    "df5_limmaout.rename(columns={'adj.P.Val':'P-Val'}, inplace=True)\n",
    "dfratiosF5 = dfratios5.iloc[:, 0:6]\n",
    "dfratiosF5[['logFC', 'P-Val', 'B']] = df5_limmaout[['logFC', 'P-Val', 'B']]\n",
    "\n",
    "#add in a new column for the gene name (duplicate protein column) and prepare the list for querying uniport\n",
    "dfratiosF5.insert(1, 'Gene', dfratiosF5.Protein)\n",
    "uniprot_input = list(dfratiosF5.Protein)\n",
    "\n",
    "#Output the number of unique proteins\n",
    "print('The number of unique proteins is:')\n",
    "len(dfratiosF5.drop_duplicates('Protein'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query uniprot to get list of gene names\n",
    "\n",
    "#define variables\n",
    "url = 'https://www.uniprot.org/uploadlists/'\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "\n",
    "#make a dictionary for the requests\n",
    "params = {\n",
    "    \"from\":\"ACC+ID\",\n",
    "    \"to\":\"ACC\",\n",
    "    \"format\":\"tab\",\n",
    "    \"columns\": \"genes(PREFERRED)\",\n",
    "    \"query\":\" \".join(uniprot_input)\n",
    "}\n",
    "\n",
    "#query uniprot\n",
    "data = urllib.parse.urlencode(params, doseq=False)\n",
    "data = data.encode('ascii')\n",
    "headers = {\"User-Agent\": user_agent}\n",
    "request = urllib.request.Request(url, data, headers)\n",
    "response = urllib.request.urlopen(request)\n",
    "with urllib.request.urlopen(request) as f:\n",
    "   response = f.read().decode()\n",
    "\n",
    "#make a dataframe from the output\n",
    "df_res = pd.read_csv(StringIO(response), sep='\\t')\n",
    "df_res #inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the gene names to the ratios table from above based on the uniprot output\n",
    "#then, normalize the ratios based on the protein expression ratios\n",
    "\n",
    "#rename the uniprot output columns\n",
    "df_res.rename(columns={df_res.columns[0]: \"Gene\", df_res.columns[1]: \"UniprotID\"}, inplace=True)\n",
    "\n",
    "#set up the dictionary for mapping\n",
    "di = df_res.set_index('UniprotID')['Gene'].to_dict()\n",
    "\n",
    "#replace the gene name column in the ratios df with the gene name based on the above dictionary\n",
    "dfratiosF5['Gene'].replace(di, inplace=True)\n",
    "\n",
    "#import the PD output for protein expression and get the gene name for merging with ratios df\n",
    "dfproteins = pd.read_csv(fnamep, sep='\\t')\n",
    "dfproteins = dfproteins[['Description', 'Abundance Ratio log2 BAP1 KO  Control']]\n",
    "dfproteins['Gene'] = dfproteins.Description.str.extract('GN=(.*) PE=')\n",
    "dfproteins.rename(columns={'Abundance Ratio log2 BAP1 KO  Control':'logFC Protein'}, inplace=True)\n",
    "dfproteins = dfproteins.groupby('Gene', as_index=False).mean()\n",
    "\n",
    "#merge with ratios file (validating that there are not duplicate gene names in the proteins file)\n",
    "dfratiosproteins5 = dfratiosF5.merge(dfproteins, how='left', on='Gene', validate='m:1')\n",
    "\n",
    "#export this as a txt file\n",
    "dfratiosproteins5.to_csv('BAP1KO_SitesRegions_Ratios.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct the sites and regions ratios for changes in protein expression and re-run limma\n",
    "\n",
    "dflimmaC5 = dflimma5.copy() #make a copy of the original ratios table\n",
    "\n",
    "#define log protein expression value to subtract\n",
    "logPE = dfratiosproteins5['logFC Protein'].fillna(0)\n",
    "\n",
    "dflimmaC5[['Trial1', 'Trial2', 'Trial3']] = \\\n",
    "dflimmaC5[['Trial1', 'Trial2', 'Trial3']] - [logPE, logPE, logPE]\n",
    "\n",
    "#export for running the limma script\n",
    "dflimmaC5.to_csv('BAP1KO_SitesRegions_RatiosCorrected_Limma.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#calculate the p-values for changing sites with limma in R\n",
    "\n",
    "library(limma) #load the package\n",
    "df1 = read.table(\"BAP1KO_SitesRegions_RatiosCorrected_Limma.txt\",sep=\"\\t\",header=TRUE,row.names=1) #read in the file\n",
    "f1 = eBayes(lmFit(df1)) #perform limma\n",
    "write.table(topTable(f1,n=Inf,confint=.95),\"BAP1KO_limmaOut_Corrected.txt\",sep=\"\\t\") #write table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the limma output back in, clean up, and export a new df with the final ratios\n",
    "#also export a df with just Gene, Protein, and Site ID Constraints of significantly changing sites for cytoscape\n",
    "\n",
    "#import and clean up limma output\n",
    "df5_limmaoutC = pd.read_csv('BAP1KO_limmaOut_Corrected.txt', sep='\\t')\n",
    "df5_limmaoutC.rename(columns={'adj.P.Val':'P-Val_Corrected', 'B':'B_Corrected', 'logFC':'logFC_Corrected'}, \n",
    "                     inplace=True)\n",
    "\n",
    "#make a copy of the previous final ratios df and add the protein expression corrected columns in\n",
    "dfcorrectedratios5 = dfratiosproteins5.copy()\n",
    "dfcorrectedratios5[['logFC_Corrected', 'P-Val_Corrected', 'B_Corrected']] = df5_limmaoutC[['logFC_Corrected', \n",
    "                                                                                           'P-Val_Corrected', \n",
    "                                                                                           'B_Corrected']]\n",
    "\n",
    "#export final list of ratios\n",
    "dfcorrectedratios5.to_csv('BAP1KO_SitesRegions_Ratios_Corrected.csv', index=False)\n",
    "\n",
    "#export table for cytoscape\n",
    "\n",
    "#get relevant columns\n",
    "dfsrRatios_ForCytoscape = dfcorrectedratios5[['Gene', 'Protein', 'Site ID Constraints', 'logFC_Corrected', \n",
    "                                              'P-Val_Corrected', 'logFC Protein']]\n",
    "\n",
    "#add column for whether protein expression is corrected for\n",
    "dfsrRatios_ForCytoscape.loc[dfsrRatios_ForCytoscape['logFC Protein'].isnull(), \n",
    "                            'Corrected for Protein Expression'] = 'No'\n",
    "dfsrRatios_ForCytoscape.loc[~dfsrRatios_ForCytoscape['logFC Protein'].isnull(), \n",
    "                            'Corrected for Protein Expression'] = 'Yes'\n",
    "\n",
    "#drop log FC protein column, add an additional column for cytoscape splicing, and combine rows with the same gene\n",
    "dfsrRatios_ForCytoscape.drop(columns={'logFC Protein'}, inplace=True)\n",
    "dfsrRatios_ForCytoscape.loc[dfcorrectedratios5['P-Val_Corrected'] < 0.05, 'Significantly Changing Site'] = 1\n",
    "\n",
    "#concatentate sites/regions together with their position and fold change in a new column\n",
    "dfsrRatios_ForCytoscape['Sites_Regions'] = (dfsrRatios_ForCytoscape['Protein'] + '@' + \n",
    "                                                dfsrRatios_ForCytoscape['Site ID Constraints'] + ' (FC: ' + \n",
    "                                                dfsrRatios_ForCytoscape['logFC_Corrected'].round(2).astype(str) +')')\n",
    "\n",
    "#calculate the average fold change per gene and save in a separate df\n",
    "dfavgFC = dfsrRatios_ForCytoscape.groupby(['Gene'])['logFC_Corrected'].mean().reset_index()\n",
    "dfavgFC.rename(columns={'logFC_Corrected':'AvgLogFC'}, inplace=True)\n",
    "\n",
    "#store the concatenated sites/regions for each gene as a separate df\n",
    "dfsrconcat = dfsrRatios_ForCytoscape.groupby(['Gene'])['Sites_Regions'].apply('; '.join).reset_index()\n",
    "dfsrconcat.rename(columns={'Sites_Regions':'O-GlcNAc Sites and Regions'}, inplace=True)\n",
    "\n",
    "#store the concatenated significant sites/regions for each gene as a separate df\n",
    "dfsigsrconcat = dfsrRatios_ForCytoscape[dfsrRatios_ForCytoscape['Significantly Changing Site'] == 1].groupby(\n",
    "    ['Gene'])['Sites_Regions'].apply('; '.join).reset_index()\n",
    "dfsigsrconcat.rename(columns={'Sites_Regions':'Significant Sites and Regions'}, inplace=True)\n",
    "\n",
    "#combine everything together in the same df\n",
    "dfsrRatios_ForCytoscape = dfsrRatios_ForCytoscape.merge(dfavgFC, how='left', on='Gene')\n",
    "dfsrRatios_ForCytoscape = dfsrRatios_ForCytoscape.merge(dfsrconcat, how='left', on='Gene')\n",
    "dfsrRatios_ForCytoscape = dfsrRatios_ForCytoscape.merge(dfsigsrconcat, how='left', on='Gene')\n",
    "\n",
    "#export for loading into cytoscape\n",
    "dfsrRatios_ForCytoscape[['Gene', 'AvgLogFC', 'Corrected for Protein Expression', 'O-GlcNAc Sites and Regions', \n",
    "                         'Significant Sites and Regions']].to_csv('BAP1KO_SitesRegions_ForCytoscape.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
